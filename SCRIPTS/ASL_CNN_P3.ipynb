{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2b1c93fa",
      "metadata": {},
      "source": [
        "## GET AND CLEAN DATA FROM DATA FOLDER BEFORE RUNNING THIS SCRIPT!!!\n",
        "\n",
        "#### I ran this code using Google Colab since the training takes awhile, but you're welcome to use any platform you like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4hWsczVk4Tir",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hWsczVk4Tir",
        "outputId": "387872db-620b-49a0-8370-44ecb1a7c4e6"
      },
      "outputs": [],
      "source": [
        "# Unzip the cleaned dataset\n",
        "!unzip \"/content/asl_data.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba36e4f2-0f18-4d7d-88fc-8bda5b9d727c",
      "metadata": {
        "id": "ba36e4f2-0f18-4d7d-88fc-8bda5b9d727c"
      },
      "outputs": [],
      "source": [
        "# Define variables for you models and dataset\n",
        "DATA_DIR = \"/content/asl_data\"         # root containing subfolders per class\n",
        "IMG_SIZE = (150, 150)\n",
        "BATCH = 64                    # try 64 first; drop to 32 if memory is tight\n",
        "EPOCHS = 20\n",
        "VAL_SPLIT = 0.2               # train/val split from the directory\n",
        "SEED = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05e0d91b-ed8c-4dbd-be01-a347f2b7079e",
      "metadata": {
        "id": "05e0d91b-ed8c-4dbd-be01-a347f2b7079e"
      },
      "outputs": [],
      "source": [
        "# Import necessary packages and tools\n",
        "import os, sys, itertools, numpy as np, tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10d4yaEQExEW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10d4yaEQExEW",
        "outputId": "d3cdf8d9-91eb-4695-8152-08260654f2b3"
      },
      "outputs": [],
      "source": [
        "# Check for bad files in the dataset\n",
        "import os\n",
        "\n",
        "bad_files = []\n",
        "\n",
        "for root, dirs, files in os.walk(DATA_DIR):\n",
        "    for f in files:\n",
        "        path = os.path.join(root, f)\n",
        "        # Check empty file\n",
        "        if os.path.getsize(path) == 0:\n",
        "            bad_files.append(path)\n",
        "\n",
        "print(\"Bad files:\", bad_files)\n",
        "print(\"Count:\", len(bad_files))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5E2-c4yYFKmP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E2-c4yYFKmP",
        "outputId": "0bf11a6e-c03e-4279-c15b-7a84738fc14b"
      },
      "outputs": [],
      "source": [
        "# Remove bad files from the dataset\n",
        "import os\n",
        "\n",
        "for path in bad_files:\n",
        "    try:\n",
        "        os.remove(path)\n",
        "        print(\"Deleted:\", path)\n",
        "    except Exception as e:\n",
        "        print(\"Error deleting\", path, e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39dc8ed9-e3e2-4ee8-b541-5f9a861f586c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39dc8ed9-e3e2-4ee8-b541-5f9a861f586c",
        "outputId": "35dcbf4f-9018-4871-ed62-7b0ef3ef0439"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training and validation sets\n",
        "\n",
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    validation_split=VAL_SPLIT,\n",
        "    subset=\"training\",\n",
        "    seed=SEED,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH,\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    DATA_DIR,\n",
        "    validation_split=VAL_SPLIT,\n",
        "    subset=\"validation\",\n",
        "    seed=SEED,\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH,\n",
        ")\n",
        "\n",
        "# Get class names and number of classes (if correctly loaded)\n",
        "class_names = train_ds.class_names\n",
        "num_classes = len(class_names)\n",
        "print(\"Classes:\", class_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sPqYRhJJ6Azt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "sPqYRhJJ6Azt",
        "outputId": "8aa88b0f-6f73-494f-93d6-673dae287b87"
      },
      "outputs": [],
      "source": [
        "# Setting up the model with Transfer Learning\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications import ResNet50V2\n",
        "from tensorflow.keras.applications.resnet_v2 import preprocess_input\n",
        "from tensorflow.keras import layers, models, regularizers, optimizers\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Use a Pre-trained Network with Transfer Learning\n",
        "# Define input shape required by ResNet50V2\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "# Create base model with pre-trained weights\n",
        "base_model = ResNet50V2(weights='imagenet',\n",
        "                        include_top=False,  # exclude the top layers since we're adding our own\n",
        "                        input_shape=input_shape)\n",
        "\n",
        "# Build the full model with custom top layers\n",
        "# Youre free to modify the architecture as needed but this is a good starting point!\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dense(512, activation='relu', kernel_regularizer=regularizers.l2(0.0005)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(26, activation='softmax')\n",
        "])\n",
        "\n",
        "# Resize and preprocess datasets\n",
        "def preprocess_dataset(ds):\n",
        "    ds = ds.map(lambda x, y: (tf.image.resize(x, [224, 224]), y))\n",
        "    ds = ds.map(lambda x, y: (preprocess_input(x), y))\n",
        "    return ds\n",
        "\n",
        "train_ds = preprocess_dataset(train_ds)\n",
        "validation_ds = preprocess_dataset(val_ds)\n",
        "\n",
        "# Setup callbacks to help reduce plateu and save best model!\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6, verbose=1\n",
        ")\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss', patience=8, restore_best_weights=True, verbose=1\n",
        ")\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    'best_resnet_model.keras',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_r6b7KLW6RqX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r6b7KLW6RqX",
        "outputId": "6cd4a0de-4d62-45a1-bdac-9db6dc097986"
      },
      "outputs": [],
      "source": [
        "# Phase 1 code here\n",
        "# Freeze the base model and train only the top layers first\n",
        "print(\"Phase 1: Training only the top layers with frozen base model...\")\n",
        "\n",
        "base_model.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-3),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_phase1 = model.fit(\n",
        "    train_ds,\n",
        "    epochs=15, # You can adjust the number of epochs as needed\n",
        "    validation_data=validation_ds,\n",
        "    callbacks=[reduce_lr, early_stop, checkpoint] # Remember to include your callbacks here!\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yy_kDgFo6Ti5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yy_kDgFo6Ti5",
        "outputId": "8f558e93-ffba-4b7c-f5b3-0cbdae2f82f4"
      },
      "outputs": [],
      "source": [
        "# Phase 2 code here\n",
        "print(\"\\nPhase 2: Fine-tuning the model with small learning rate...\")\n",
        "\n",
        "# Unfreeze the base model, but keep BatchNorm layers frozen\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers:\n",
        "    if isinstance(layer, layers.BatchNormalization):\n",
        "        layer.trainable = False\n",
        "\n",
        "# Compile model for Phase 2 with much smaller learning rate as we approach final convergence\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train Phase 2\n",
        "history_phase2 = model.fit(\n",
        "    train_ds,\n",
        "    epochs=15,\n",
        "    validation_data=validation_ds,\n",
        "    callbacks=[reduce_lr, early_stop, checkpoint]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4GrMU_oH6XeO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 805
        },
        "id": "4GrMU_oH6XeO",
        "outputId": "c2d2c786-2d6e-493c-b475-dc92e760216a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "# Load the best model and evaluate\n",
        "best_model = tf.keras.models.load_model('best_resnet_model.keras')\n",
        "evaluation = best_model.evaluate(validation_ds)\n",
        "print(f\"Final validation accuracy: {evaluation[1]:.4f}\")\n",
        "\n",
        "# Get true and predicted labels\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels in validation_ds:\n",
        "    preds = best_model.predict(images)\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "# Display Confusion Matrix to see detailed performance\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(xticks_rotation=90)\n",
        "plt.title(\"Confusion Matrix - Validation Set\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
